from openai import OpenAI
import os
import logging
from typing import Any, Optional, Sequence, Union, Mapping

from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.language_models.base import LanguageModelInput
from langchain_core.messages import AIMessage, SystemMessage, BaseMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import BaseTool
from langchain_core.utils.function_calling import convert_to_openai_tool

from src.utils import config

logger = logging.getLogger(__name__)


class ZKHChatOpenAI(ChatOpenAI):
    """
    ZKH AI Gateway ä¸“ç”¨çš„ ChatOpenAI å­ç±»
    
    ZKH API ä¸æ”¯æŒä»¥ä¸‹å‚æ•°ï¼Œéœ€è¦è¿‡æ»¤ï¼š
    - tool_choice çš„å¯¹è±¡æ ¼å¼ {"type": "function", "function": {"name": "..."}}
    - parallel_tool_calls
    - response_format
    - strict (åœ¨ tools ä¸­)
    """
    
    def _filter_unsupported_params(self, kwargs: dict) -> dict:
        """è¿‡æ»¤ ZKH ä¸æ”¯æŒçš„å‚æ•°"""
        filtered = kwargs.copy()
        
        # ç§»é™¤ä¸æ”¯æŒçš„é¡¶çº§å‚æ•°
        unsupported_params = ['parallel_tool_calls', 'response_format', 'stream_options']
        for param in unsupported_params:
            if param in filtered:
                logger.debug(f"ZKH: Removing unsupported parameter '{param}'")
                del filtered[param]
        
        # å¤„ç† tool_choice: åªæ”¯æŒå­—ç¬¦ä¸²æ ¼å¼ ("auto", "required", "none")
        if 'tool_choice' in filtered:
            tool_choice = filtered['tool_choice']
            if isinstance(tool_choice, dict):
                # å°†å¯¹è±¡æ ¼å¼è½¬æ¢ä¸º "required"
                logger.debug(f"ZKH: Converting tool_choice from {tool_choice} to 'required'")
                filtered['tool_choice'] = 'required'
        
        # å¤„ç† tools: ç§»é™¤ strict å‚æ•°
        if 'tools' in filtered and filtered['tools']:
            cleaned_tools = []
            for tool in filtered['tools']:
                if isinstance(tool, dict):
                    tool_copy = tool.copy()
                    if 'function' in tool_copy and isinstance(tool_copy['function'], dict):
                        func = tool_copy['function'].copy()
                        if 'strict' in func:
                            del func['strict']
                        tool_copy['function'] = func
                    cleaned_tools.append(tool_copy)
                else:
                    cleaned_tools.append(tool)
            filtered['tools'] = cleaned_tools
        
        return filtered
    
    def _generate(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> Any:
        """é‡å†™ _generate æ–¹æ³•ï¼Œåœ¨è°ƒç”¨å‰è¿‡æ»¤å‚æ•°"""
        filtered_kwargs = self._filter_unsupported_params(kwargs)
        return super()._generate(messages, stop, run_manager, **filtered_kwargs)
    
    async def _agenerate(
        self,
        messages: list[BaseMessage],
        stop: Optional[list[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> Any:
        """é‡å†™ _agenerate æ–¹æ³•ï¼Œåœ¨è°ƒç”¨å‰è¿‡æ»¤å‚æ•°"""
        filtered_kwargs = self._filter_unsupported_params(kwargs)
        return await super()._agenerate(messages, stop, run_manager, **filtered_kwargs)


class DeepSeekR1ChatOpenAI(ChatOpenAI):
    """æ”¯æŒ DeepSeek R1 æ¨ç†æ¨¡å¼çš„ ChatOpenAI å­ç±»"""

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.client = OpenAI(
            base_url=kwargs.get("base_url"),
            api_key=kwargs.get("api_key")
        )

    async def ainvoke(
            self,
            input: LanguageModelInput,
            config: Optional[RunnableConfig] = None,
            *,
            stop: Optional[list[str]] = None,
            **kwargs: Any,
    ) -> AIMessage:
        message_history = []
        for input_ in input:
            if isinstance(input_, SystemMessage):
                message_history.append({"role": "system", "content": input_.content})
            elif isinstance(input_, AIMessage):
                message_history.append({"role": "assistant", "content": input_.content})
            else:
                message_history.append({"role": "user", "content": input_.content})

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=message_history
        )

        reasoning_content = getattr(response.choices[0].message, 'reasoning_content', None)
        content = response.choices[0].message.content
        return AIMessage(content=content, reasoning_content=reasoning_content)

    def invoke(
            self,
            input: LanguageModelInput,
            config: Optional[RunnableConfig] = None,
            *,
            stop: Optional[list[str]] = None,
            **kwargs: Any,
    ) -> AIMessage:
        message_history = []
        for input_ in input:
            if isinstance(input_, SystemMessage):
                message_history.append({"role": "system", "content": input_.content})
            elif isinstance(input_, AIMessage):
                message_history.append({"role": "assistant", "content": input_.content})
            else:
                message_history.append({"role": "user", "content": input_.content})

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=message_history
        )

        reasoning_content = getattr(response.choices[0].message, 'reasoning_content', None)
        content = response.choices[0].message.content
        return AIMessage(content=content, reasoning_content=reasoning_content)


class DeepSeekR1ChatOllama(ChatOllama):
    """æ”¯æŒæœ¬åœ° DeepSeek R1 æ¨ç†æ¨¡å¼çš„ ChatOllama å­ç±»"""

    async def ainvoke(
            self,
            input: LanguageModelInput,
            config: Optional[RunnableConfig] = None,
            *,
            stop: Optional[list[str]] = None,
            **kwargs: Any,
    ) -> AIMessage:
        org_ai_message = await super().ainvoke(input=input)
        org_content = org_ai_message.content
        if "</think>" in org_content:
            reasoning_content = org_content.split("</think>")[0].replace("<think>", "")
            content = org_content.split("</think>")[1]
            if "**JSON Response:**" in content:
                content = content.split("**JSON Response:**")[-1]
        else:
            reasoning_content = None
            content = org_content
        return AIMessage(content=content, reasoning_content=reasoning_content)

    def invoke(
            self,
            input: LanguageModelInput,
            config: Optional[RunnableConfig] = None,
            *,
            stop: Optional[list[str]] = None,
            **kwargs: Any,
    ) -> AIMessage:
        org_ai_message = super().invoke(input=input)
        org_content = org_ai_message.content
        if "</think>" in org_content:
            reasoning_content = org_content.split("</think>")[0].replace("<think>", "")
            content = org_content.split("</think>")[1]
            if "**JSON Response:**" in content:
                content = content.split("**JSON Response:**")[-1]
        else:
            reasoning_content = None
            content = org_content
        return AIMessage(content=content, reasoning_content=reasoning_content)


def get_llm_model(provider: str, **kwargs):
    """
    è·å– LLM æ¨¡å‹å®ä¾‹
    
    æ”¯æŒçš„ provider:
    - zkh: ZKH AI Gateway (é»˜è®¤)
    - openai: OpenAI
    - deepseek: DeepSeek
    - ollama: Ollama æœ¬åœ°æ¨¡å‹
    
    :param provider: LLM ä¾›åº”å•†åç§°
    :param kwargs: æ¨¡å‹é…ç½®å‚æ•°
    :return: LLM æ¨¡å‹å®ä¾‹
    """
    # Ollama ä¸éœ€è¦ API Key
    if provider != "ollama":
        env_var = f"{provider.upper()}_API_KEY"
        api_key = kwargs.get("api_key", "") or os.getenv(env_var, "")
        if not api_key:
            provider_display = config.PROVIDER_DISPLAY_NAMES.get(provider, provider.upper())
            error_msg = f"ğŸ’¥ {provider_display} API key not found! ğŸ”‘ Please set the `{env_var}` environment variable or provide it in the UI."
            raise ValueError(error_msg)
        kwargs["api_key"] = api_key
    
    # ==================== ZKH AI Gateway (é»˜è®¤ä¾›åº”å•†) ====================
    if provider == "zkh":
        # ZKH API å…¼å®¹ OpenAI æ ¼å¼ï¼Œä½†æœ‰éƒ¨åˆ†å‚æ•°é™åˆ¶
        base_url = kwargs.get("base_url") or os.getenv("ZKH_ENDPOINT", "https://ai-dev-gateway.zkh360.com/llm/v1")
        
        return ZKHChatOpenAI(
            model=kwargs.get("model_name", "ep_20250815_yc11"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=base_url,
            api_key=api_key,
        )
    
    # ==================== OpenAI ====================
    elif provider == "openai":
        base_url = kwargs.get("base_url") or os.getenv("OPENAI_ENDPOINT", "https://api.openai.com/v1")
        
        return ChatOpenAI(
            model=kwargs.get("model_name", "gpt-4o"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=base_url,
            api_key=api_key,
        )
    
    # ==================== DeepSeek ====================
    elif provider == "deepseek":
        base_url = kwargs.get("base_url") or os.getenv("DEEPSEEK_ENDPOINT", "https://api.deepseek.com")
        
        # DeepSeek Reasoner æ¨¡å‹æ”¯æŒæ¨ç†å†…å®¹
        if kwargs.get("model_name", "deepseek-chat") == "deepseek-reasoner":
            return DeepSeekR1ChatOpenAI(
                model=kwargs.get("model_name", "deepseek-reasoner"),
                temperature=kwargs.get("temperature", 0.0),
                base_url=base_url,
                api_key=api_key,
            )
        else:
            return ChatOpenAI(
                model=kwargs.get("model_name", "deepseek-chat"),
                temperature=kwargs.get("temperature", 0.0),
                base_url=base_url,
                api_key=api_key,
            )
    
    # ==================== Ollama (æœ¬åœ°æ¨¡å‹) ====================
    elif provider == "ollama":
        base_url = kwargs.get("base_url") or os.getenv("OLLAMA_ENDPOINT", "http://localhost:11434")
        
        # æœ¬åœ° DeepSeek R1 æ¨¡å‹æ”¯æŒæ¨ç†å†…å®¹
        if "deepseek-r1" in kwargs.get("model_name", "qwen2.5:7b"):
            return DeepSeekR1ChatOllama(
                model=kwargs.get("model_name", "deepseek-r1:14b"),
                temperature=kwargs.get("temperature", 0.0),
                num_ctx=kwargs.get("num_ctx", 32000),
                base_url=base_url,
            )
        else:
            return ChatOllama(
                model=kwargs.get("model_name", "qwen2.5:7b"),
                temperature=kwargs.get("temperature", 0.0),
                num_ctx=kwargs.get("num_ctx", 32000),
                num_predict=kwargs.get("num_predict", 1024),
                base_url=base_url,
            )
    
    else:
        raise ValueError(f"Unsupported provider: {provider}. Supported providers: zkh, openai, deepseek, ollama")
